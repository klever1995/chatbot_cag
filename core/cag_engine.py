import logging
from models.llms import azure_llm_client
from cache.context_cache import context_cache
from cache.redis_manager import redis_cache

logger = logging.getLogger(__name__)

class CAGEngine:
    """Motor CAG para respuestas eficientes desde cach√© de contexto y conversaci√≥n general"""
    
    def __init__(self):
        logger.info("‚úÖ Motor CAG inicializado")
    
    def generate_response(self, query, is_general_conversation=False):
        """Genera una respuesta usando el contexto cacheado o conversaci√≥n general"""
        try:
            # Conversaci√≥n general (sin cach√© de documentos)
            if is_general_conversation:
                logger.debug(f"üí¨ Modo conversaci√≥n general: {query}")
                return self._generate_general_response(query)
            
            #B√∫squeda en cach√© de contexto documental
            logger.debug(f"üîç Buscando en cach√© de contexto: {query}")
            context_results = context_cache.search_in_cache(query)
            
            if not context_results:
                return "No tengo informaci√≥n suficiente en mi conocimiento almacenado para responder esta pregunta."
            
            # Construir contexto con los documentos encontrados
            context = self._build_context(context_results)
            
            # Generar prompt para el LLM con contexto documental
            messages = self._build_document_context_messages(query, context)
            
            # Obtener respuesta del LLM optimizada para contexto legal
            logger.debug("Generando respuesta legal con Azure OpenAI...")
            response = azure_llm_client.generate_legal_response(messages)
            
            # Cachear respuesta en Redis
            redis_cache.set_cached_response(query, response)
            
            logger.debug("‚úÖ Respuesta CAG generada exitosamente")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Error en el motor CAG: {str(e)}")
            return "Lo siento, ocurri√≥ un error al procesar tu consulta."
    
    def _generate_general_response(self, query):
        """Genera respuestas para conversaci√≥n general"""
        general_messages = [
            {
                "role": "system", 
                "content": """Eres un asistente amable y √∫til especializado en temas laborales y legales. 
Responde preguntas generales de manera cort√©s pero mant√©n el foco en tu √°rea de expertise.
Para preguntas muy personales o fuera de contexto, redirige amablemente a temas laborales.
Mant√©n respuestas breves y naturales."""
            },
            {
                "role": "user", 
                "content": query
            }
        ]
        
        try:
            # M√©todo optimizado para conversaci√≥n
            response = azure_llm_client.generate_conversational_response(general_messages)
            redis_cache.set_cached_response(query, response)
            return response
        except Exception as e:
            logger.error(f"‚ùå Error en respuesta general: {str(e)}")
            return "¬°Hola! Soy un asistente especializado en temas laborales y documentaci√≥n. ¬øEn qu√© puedo ayudarte hoy?"
    
    def _build_context(self, context_results):
        """Construye el contexto a partir de los resultados de cach√©"""
        if not context_results:
            return "No se encontr√≥ contexto relevante."
        
        context = "üìö CONTEXTO ENCONTRADO EN KNOWLEDGE BASE:\n\n"
        
        for i, result in enumerate(context_results, 1):
            similarity = result.get('similarity', 0)
            context += f"--- Documento {i} (Relevancia: {similarity:.0%}) ---\n"
            context += f"{result['document']}\n\n"
        
        return context
    
    def _build_document_context_messages(self, query, context):
        """Construye los mensajes para el LLM con el contexto cacheado"""
        system_message = """Eres un asistente legal/laboral especializado. Responde preguntas bas√°ndote √öNICAMENTE en el contexto proporcionado.

REGLAS ESTRICTAS:
1. Responde EXCLUSIVAMENTE con informaci√≥n del contexto proporcionado
2. Si la informaci√≥n no est√° en el contexto, di: "No tengo informaci√≥n sobre esto en mi knowledge base"
3. S√© preciso, conciso y profesional
4. Usa el mismo idioma de la pregunta
5. Cita la relevancia del documento cuando sea apropiado
6. Mant√©n un tono profesional pero accesible

IMPORTANTE: Nunca inventes informaci√≥n o hagas suposiciones fuera del contexto."""

        user_message = f"""CONTEXTO DE KNOWLEDGE BASE:
{context}

PREGUNTA: {query}

INSTRUCCI√ìN: Responde la pregunta bas√°ndote √öNICAMENTE en el contexto anterior. Si la respuesta no est√° en el contexto, ind√≠calo claramente."""

        return [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
    
    def _handle_empty_context(self, query):
        """Maneja el caso cuando no se encuentra contexto relevante"""
        no_context_messages = [
            {
                "role": "system",
                "content": """Eres un asistente especializado en temas laborales. 
Cuando no tienes informaci√≥n en tu knowledge base, responde amablemente indicando esto y ofrece ayuda dentro de tu √°rea de expertise."""
            },
            {
                "role": "user",
                "content": f"Pregunta: {query}\n\nInstrucci√≥n: Responde indicando que no tienes informaci√≥n espec√≠fica sobre esto en tu knowledge base, pero mant√©n un tono √∫til."
            }
        ]
        
        try:
            return azure_llm_client.generate_conversational_response(no_context_messages)
        except Exception:
            return "No tengo informaci√≥n espec√≠fica sobre esto en mi knowledge base. ¬øPuedo ayudarte con alguna otra consulta laboral?"

    def process_query(self, query, is_general_conversation=False):
        """M√©todo principal para procesar consultas CAG"""
        return self.generate_response(query, is_general_conversation)

# Instancia singleton para ser importada
cag_engine = CAGEngine()