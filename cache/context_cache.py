# cache/context_cache.py
import logging
import hashlib
from datetime import datetime, timedelta
import chromadb
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class ContextCache:
    """Cach√© de contexto sem√°ntica para documentos legales/laborales usando ChromaDB"""
    
    def __init__(self):
        try:
            # Inicializar ChromaDB para b√∫squeda sem√°ntica
            self.client = chromadb.PersistentClient(path="./context_cache_db")
            self.collection = self.client.get_or_create_collection(
                name="legal_context_cache",
                metadata={"hnsw:space": "cosine", "description": "Cach√© de contexto para documentos legales"}
            )
            
            # Modelo de embeddings para b√∫squeda sem√°ntica
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("‚úÖ Cach√© de contexto sem√°ntica inicializada con ChromaDB")
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando cach√© de contexto: {str(e)}")
            raise
    
    def _generate_hash(self, content):
        """Genera un hash √∫nico para el contenido del documento"""
        return hashlib.md5(content.encode()).hexdigest()
    
    def add_document(self, document_content, doc_type="legal", source="uploaded"):
        """A√±ade un documento a la cach√© de contexto con embeddings sem√°nticos"""
        try:
            if not document_content or len(document_content.strip()) < 50:
                logger.warning("‚ö†Ô∏è Contenido muy corto para a√±adir a cach√©")
                return None
            
            doc_hash = self._generate_hash(document_content)
            
            # Generar embedding sem√°ntico
            embedding = self.embedding_model.encode(document_content).tolist()
            
            metadata = {
                "doc_type": doc_type,
                "source": source,
                "added_at": datetime.now().isoformat(),
                "length": len(document_content),
                "hash": doc_hash
            }
            
            # A√±adir a ChromaDB con embedding
            self.collection.add(
                ids=[doc_hash],
                documents=[document_content],
                metadatas=[metadata],
                embeddings=[embedding]
            )
            
            logger.debug(f"‚úÖ Documento a√±adido a cach√© sem√°ntica: {doc_hash[:8]}...")
            return doc_hash
            
        except Exception as e:
            logger.error(f"‚ùå Error a√±adiendo documento a cach√© sem√°ntica: {str(e)}")
            return None
    
    def search_in_cache(self, query, top_k=3, similarity_threshold=0.6):
        """
        Busca documentos en la cach√© usando b√∫squeda sem√°ntica
        Returns:
            list: Documentos relevantes ordenados por similitud
        """
        try:
            if not query or len(query.strip()) < 3:
                return []
            
            # Generar embedding de la consulta
            query_embedding = self.embedding_model.encode(query).tolist()
            
            # Buscar en ChromaDB
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["documents", "metadatas", "distances"]
            )
            
            # Filtrar por threshold de similitud
            relevant_results = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results["documents"][0], 
                results["metadatas"][0], 
                results["distances"][0]
            )):
                similarity = 1 - distance  # Convertir distancia a similitud
                
                if similarity >= similarity_threshold:
                    relevant_results.append({
                        "document": doc,
                        "metadata": metadata,
                        "similarity": round(similarity, 3),
                        "rank": i + 1
                    })
            
            logger.debug(f"‚úÖ B√∫squeda sem√°ntica: {len(relevant_results)} resultados relevantes")
            return relevant_results
            
        except Exception as e:
            logger.error(f"‚ùå Error en b√∫squeda sem√°ntica: {str(e)}")
            return []
    
    def get_context_stats(self):
        """Obtiene estad√≠sticas de la cach√© de contexto"""
        try:
            stats = self.collection.count()
            return {
                "total_documents": stats,
                "status": "active"
            }
        except Exception as e:
            logger.error(f"‚ùå Error obteniendo estad√≠sticas: {str(e)}")
            return {"total_documents": 0, "status": "error"}
    
    def clear_old_entries(self, max_age_days=30):
        """Limpia entradas antiguas de la cach√© (implementaci√≥n b√°sica)"""
        try:
            count = self.collection.count()
            logger.info(f"üîÑ Cach√© contiene {count} documentos (limpieza manual requerida)")
            return count
            
        except Exception as e:
            logger.error(f"‚ùå Error en limpieza de cach√©: {str(e)}")
            return 0
    
    def get_document_by_hash(self, doc_hash):
        """Recupera un documento espec√≠fico por su hash"""
        try:
            results = self.collection.get(ids=[doc_hash])
            if results["documents"]:
                return {
                    "document": results["documents"][0],
                    "metadata": results["metadatas"][0]
                }
            return None
        except Exception as e:
            logger.error(f"‚ùå Error recuperando documento: {str(e)}")
            return None

# Instancia singleton para ser importada
context_cache = ContextCache()